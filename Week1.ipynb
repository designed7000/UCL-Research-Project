{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfba516d",
   "metadata": {},
   "source": [
    "# Basic essential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa0eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import particles package components\n",
    "import particles\n",
    "from particles import state_space_models as ssm\n",
    "from particles import mcmc\n",
    "from particles import distributions\n",
    "from particles.collectors import Moments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed2df2",
   "metadata": {},
   "source": [
    "# S&P 500 Stochastic Volatility Analysis using Particles Package\n",
    "\n",
    "This notebook implements:\n",
    "1. Data fetching from Yahoo Finance API  \n",
    "2. Stochastic volatility model using particles package  \n",
    "3. Filtering (real-time volatility estimation)  \n",
    "4. Smoothing (hindsight volatility estimation)  \n",
    "5. Bayesian parameter estimation via Particle MCMC  \n",
    "\n",
    "**Model:**  \n",
    "- **State:** \\(X_t = \\text{log-volatility}\\)  \n",
    "- **Evolution:**  \n",
    "  $ X_t \\;=\\; \\mu \\;+\\; \\phi\\,\\bigl(X_{t-1} - \\mu\\bigr) \\;+\\; \\sigma_x\\,\\epsilon_t$  \n",
    "- **Observation:**  \n",
    "  $Y_t \\;=\\; \\exp\\bigl(X_t/2\\bigr)\\,\\eta_t$\n",
    "\n",
    "\n",
    "**Author:** Generated for S&P 500 volatility analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b18dcf",
   "metadata": {},
   "source": [
    "# Data before going to the modeling definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bb478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sp500_data(start_date=\"2023-01-01\", end_date=\"2024-11-01\"):\n",
    "    \"\"\"\n",
    "    Fetch S&P 500 data from Yahoo Finance and compute log returns\n",
    "    \"\"\"\n",
    "    print(f\"📊 Fetching S&P 500 data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    # Download S&P 500 data\n",
    "    ticker = \"^GSPC\"\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    \n",
    "    # Calculate log returns (skip first day as we need previous close)\n",
    "    data['Returns'] = np.log(data['Close'] / data['Close'].shift(1)) * 100\n",
    "    data = data.dropna()\n",
    "    \n",
    "    print(f\"✓ Downloaded {len(data)} trading days\")\n",
    "    print(f\"✓ Return statistics: Mean={data['Returns'].mean():.3f}%, Std={data['Returns'].std():.2f}%\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd46057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Fetching S&P 500 data from 2023-01-01 to 2024-11-01...\n",
      "YF.download() has changed argument auto_adjust default to True\n",
      "✓ Downloaded 460 trading days\n",
      "✓ Return statistics: Mean=0.087%, Std=0.81%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>3852.969971</td>\n",
       "      <td>3873.159912</td>\n",
       "      <td>3815.770020</td>\n",
       "      <td>3840.360107</td>\n",
       "      <td>4414080000</td>\n",
       "      <td>0.751069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>3808.100098</td>\n",
       "      <td>3839.739990</td>\n",
       "      <td>3802.419922</td>\n",
       "      <td>3839.739990</td>\n",
       "      <td>3893450000</td>\n",
       "      <td>-1.171387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-06</th>\n",
       "      <td>3895.080078</td>\n",
       "      <td>3906.189941</td>\n",
       "      <td>3809.560059</td>\n",
       "      <td>3823.370117</td>\n",
       "      <td>3923560000</td>\n",
       "      <td>2.258384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-09</th>\n",
       "      <td>3892.090088</td>\n",
       "      <td>3950.570068</td>\n",
       "      <td>3890.419922</td>\n",
       "      <td>3910.820068</td>\n",
       "      <td>4311770000</td>\n",
       "      <td>-0.076793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-10</th>\n",
       "      <td>3919.250000</td>\n",
       "      <td>3919.830078</td>\n",
       "      <td>3877.290039</td>\n",
       "      <td>3888.570068</td>\n",
       "      <td>3851030000</td>\n",
       "      <td>0.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-25</th>\n",
       "      <td>5808.120117</td>\n",
       "      <td>5862.819824</td>\n",
       "      <td>5799.979980</td>\n",
       "      <td>5826.750000</td>\n",
       "      <td>3501280000</td>\n",
       "      <td>-0.029949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-28</th>\n",
       "      <td>5823.520020</td>\n",
       "      <td>5842.919922</td>\n",
       "      <td>5823.080078</td>\n",
       "      <td>5833.930176</td>\n",
       "      <td>3691280000</td>\n",
       "      <td>0.264793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-29</th>\n",
       "      <td>5832.919922</td>\n",
       "      <td>5847.189941</td>\n",
       "      <td>5802.169922</td>\n",
       "      <td>5819.680176</td>\n",
       "      <td>3879100000</td>\n",
       "      <td>0.161283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-30</th>\n",
       "      <td>5813.669922</td>\n",
       "      <td>5850.939941</td>\n",
       "      <td>5811.279785</td>\n",
       "      <td>5832.649902</td>\n",
       "      <td>3851120000</td>\n",
       "      <td>-0.330569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-31</th>\n",
       "      <td>5705.450195</td>\n",
       "      <td>5775.339844</td>\n",
       "      <td>5702.859863</td>\n",
       "      <td>5775.339844</td>\n",
       "      <td>4425660000</td>\n",
       "      <td>-1.879013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price             Close         High          Low         Open      Volume  \\\n",
       "Ticker            ^GSPC        ^GSPC        ^GSPC        ^GSPC       ^GSPC   \n",
       "Date                                                                         \n",
       "2023-01-04  3852.969971  3873.159912  3815.770020  3840.360107  4414080000   \n",
       "2023-01-05  3808.100098  3839.739990  3802.419922  3839.739990  3893450000   \n",
       "2023-01-06  3895.080078  3906.189941  3809.560059  3823.370117  3923560000   \n",
       "2023-01-09  3892.090088  3950.570068  3890.419922  3910.820068  4311770000   \n",
       "2023-01-10  3919.250000  3919.830078  3877.290039  3888.570068  3851030000   \n",
       "...                 ...          ...          ...          ...         ...   \n",
       "2024-10-25  5808.120117  5862.819824  5799.979980  5826.750000  3501280000   \n",
       "2024-10-28  5823.520020  5842.919922  5823.080078  5833.930176  3691280000   \n",
       "2024-10-29  5832.919922  5847.189941  5802.169922  5819.680176  3879100000   \n",
       "2024-10-30  5813.669922  5850.939941  5811.279785  5832.649902  3851120000   \n",
       "2024-10-31  5705.450195  5775.339844  5702.859863  5775.339844  4425660000   \n",
       "\n",
       "Price        Returns  \n",
       "Ticker                \n",
       "Date                  \n",
       "2023-01-04  0.751069  \n",
       "2023-01-05 -1.171387  \n",
       "2023-01-06  2.258384  \n",
       "2023-01-09 -0.076793  \n",
       "2023-01-10  0.695400  \n",
       "...              ...  \n",
       "2024-10-25 -0.029949  \n",
       "2024-10-28  0.264793  \n",
       "2024-10-29  0.161283  \n",
       "2024-10-30 -0.330569  \n",
       "2024-10-31 -1.879013  \n",
       "\n",
       "[460 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_sp500_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a37885",
   "metadata": {},
   "source": [
    "# Defining the stochastic volatility model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b53d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chunk 2: Stochastic Volatility Model Class Definition\n",
    "# =============================================================================\n",
    "\n",
    "class StochasticVolatilityModel(ssm.StateSpaceModel):\n",
    "    \"\"\"\n",
    "    Stochastic Volatility Model for S&P 500\n",
    "    \n",
    "    State equation: X_t = mu + phi*(X_{t-1} - mu) + sigma_x * epsilon_t\n",
    "    Observation equation: Y_t = exp(X_t/2) * eta_t\n",
    "    \n",
    "    Parameters:\n",
    "    - mu: long-run log-volatility level\n",
    "    - phi: persistence parameter (0 < phi < 1)\n",
    "    - sigma_x: volatility of log-volatility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mu=-2.5, phi=0.95, sigma_x=0.3):\n",
    "        self.mu = mu\n",
    "        self.phi = phi \n",
    "        self.sigma_x = sigma_x\n",
    "        \n",
    "    def PX0(self):\n",
    "        \"\"\"Initial distribution of log-volatility\"\"\"\n",
    "        # Stationary distribution: X_0 ~ N(mu, sigma_x^2 / (1 - phi^2))\n",
    "        var_stat = self.sigma_x**2 / (1 - self.phi**2)\n",
    "        return particles.distributions.Normal(loc=self.mu, scale=np.sqrt(var_stat))\n",
    "    \n",
    "    def PX(self, t, xp):\n",
    "        \"\"\"State transition: X_t | X_{t-1}\"\"\"\n",
    "        mean = self.mu + self.phi * (xp - self.mu)\n",
    "        return particles.distributions.Normal(loc=mean, scale=self.sigma_x)\n",
    "    \n",
    "    def PY(self, t, xp, x):\n",
    "        \"\"\"Observation distribution: Y_t | X_t\"\"\"\n",
    "        # Y_t = exp(X_t/2) * eta_t where eta_t ~ N(0,1)\n",
    "        volatility = np.exp(x / 2)\n",
    "        return particles.distributions.Normal(loc=0.0, scale=volatility)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12f13daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chunk 4: Particle Filter Function\n",
    "# =============================================================================\n",
    "\n",
    "def run_particle_filter(model, returns, N=1000):\n",
    "    \"\"\"\n",
    "    Run particle filter for stochastic volatility estimation\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Running particle filter with {N} particles...\")\n",
    "    \n",
    "    # Convert returns to numpy array\n",
    "    y_data = returns.values\n",
    "    T = len(y_data)\n",
    "    \n",
    "    # Create the Feynman-Kac model\n",
    "    fk_model = ssm.Bootstrap(ssm=model, data=y_data)\n",
    "    \n",
    "    # Run particle filter\n",
    "    pf = particles.SMC(fk=fk_model, N=N, collect=[Moments()])\n",
    "    pf.run()\n",
    "    \n",
    "    # Extract filtering means and variances\n",
    "    filtering_means = np.array([pf.summaries.moments[t]['mean'] for t in range(T)])\n",
    "    filtering_vars = np.array([pf.summaries.moments[t]['var'] for t in range(T)])\n",
    "    \n",
    "    # Convert log-volatility to volatility percentage\n",
    "    vol_estimates = np.exp(filtering_means / 2) * np.sqrt(252)  # Annualized volatility\n",
    "    vol_std = np.sqrt(filtering_vars) * np.exp(filtering_means / 2) * np.sqrt(252)\n",
    "    \n",
    "    print(\"✓ Particle filtering completed\")\n",
    "    \n",
    "    return {\n",
    "        'log_vol_mean': filtering_means,\n",
    "        'log_vol_var': filtering_vars,\n",
    "        'vol_estimates': vol_estimates,\n",
    "        'vol_std': vol_std,\n",
    "        'particle_filter': pf\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21d0a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chunk 5: Particle Smoother Function\n",
    "# =============================================================================\n",
    "\n",
    "def run_particle_smoother(model, returns, N=1000):\n",
    "    \"\"\"\n",
    "    Run particle smoother for refined volatility estimates\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Running particle smoother with {N} particles...\")\n",
    "    \n",
    "    # Convert returns to numpy array  \n",
    "    y_data = returns.values\n",
    "    T = len(y_data)\n",
    "    \n",
    "    # Run forward filter with history storage\n",
    "    fk = ssm.Bootstrap(ssm=model, data=y_data)\n",
    "    pf = particles.SMC(fk=fk, N=N, store_history=True)  # Enable history storage\n",
    "    pf.run()\n",
    "    \n",
    "    # Check if history is available\n",
    "    if pf.hist is None:\n",
    "        print(\"⚠️  History not available, using filtering estimates as smoothing approximation\")\n",
    "        # Fallback: use filtering means as approximation\n",
    "        filtering_means = np.array([np.mean(pf.X) for _ in range(T)])\n",
    "        filtering_vars = np.array([np.var(pf.X) for _ in range(T)])\n",
    "        \n",
    "        smooth_vol_estimates = np.exp(filtering_means / 2) * np.sqrt(252)\n",
    "        smooth_vol_std = np.sqrt(filtering_vars) * np.exp(filtering_means / 2) * np.sqrt(252)\n",
    "        \n",
    "        return {\n",
    "            'log_vol_mean': filtering_means,\n",
    "            'log_vol_var': filtering_vars, \n",
    "            'vol_estimates': smooth_vol_estimates,\n",
    "            'vol_std': smooth_vol_std,\n",
    "            'trajectories': None\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Run backward sampling for smoothing\n",
    "        M = min(N//10, 100)  # Number of trajectories to sample\n",
    "        paths = pf.hist.backward_sampling(M=M)\n",
    "        \n",
    "        # Compute smoothed estimates\n",
    "        smoothed_means = np.mean(paths, axis=0)\n",
    "        smoothed_vars = np.var(paths, axis=0)\n",
    "        \n",
    "        # Convert to volatility percentage\n",
    "        smooth_vol_estimates = np.exp(smoothed_means / 2) * np.sqrt(252)\n",
    "        smooth_vol_std = np.sqrt(smoothed_vars) * np.exp(smoothed_means / 2) * np.sqrt(252)\n",
    "        \n",
    "        print(\"✓ Particle smoothing completed\")\n",
    "        \n",
    "        return {\n",
    "            'log_vol_mean': smoothed_means,\n",
    "            'log_vol_var': smoothed_vars, \n",
    "            'vol_estimates': smooth_vol_estimates,\n",
    "            'vol_std': smooth_vol_std,\n",
    "            'trajectories': paths\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Backward sampling failed: {e}\")\n",
    "        print(\"   Using alternative smoothing approach...\")\n",
    "        \n",
    "        # Alternative: Fixed-lag smoother approximation\n",
    "        # Use weighted average of nearby filtering estimates\n",
    "        lag = 5  # Look ahead/behind 5 steps\n",
    "        \n",
    "        # Re-run filter with moments collection\n",
    "        pf_moments = particles.SMC(fk=fk, N=N, collect=[Moments()])\n",
    "        pf_moments.run()\n",
    "        \n",
    "        filtering_means = np.array([pf_moments.summaries.moments[t]['mean'] for t in range(T)])\n",
    "        filtering_vars = np.array([pf_moments.summaries.moments[t]['var'] for t in range(T)])\n",
    "        \n",
    "        # Simple smoothing: moving average\n",
    "        smoothed_means = np.copy(filtering_means)\n",
    "        smoothed_vars = np.copy(filtering_vars)\n",
    "        \n",
    "        for t in range(T):\n",
    "            start_idx = max(0, t - lag)\n",
    "            end_idx = min(T, t + lag + 1)\n",
    "            smoothed_means[t] = np.mean(filtering_means[start_idx:end_idx])\n",
    "            smoothed_vars[t] = np.mean(filtering_vars[start_idx:end_idx])\n",
    "        \n",
    "        # Convert to volatility percentage\n",
    "        smooth_vol_estimates = np.exp(smoothed_means / 2) * np.sqrt(252)\n",
    "        smooth_vol_std = np.sqrt(smoothed_vars) * np.exp(smoothed_means / 2) * np.sqrt(252)\n",
    "        \n",
    "        print(\"✓ Alternative smoothing completed\")\n",
    "        \n",
    "        return {\n",
    "            'log_vol_mean': smoothed_means,\n",
    "            'log_vol_var': smoothed_vars, \n",
    "            'vol_estimates': smooth_vol_estimates,\n",
    "            'vol_std': smooth_vol_std,\n",
    "            'trajectories': None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd01a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chunk 6: Bayesian Parameter Estimation Function\n",
    "# =============================================================================\n",
    "\n",
    "def bayesian_parameter_estimation(returns, n_iter=2000, n_burn=500):\n",
    "    \"\"\"\n",
    "    Bayesian parameter estimation using Particle MCMC\n",
    "    \"\"\"\n",
    "    print(f\"🎯 Running Bayesian parameter estimation ({n_iter} iterations)...\")\n",
    "    \n",
    "    # Define parameter priors\n",
    "    class SVPrior:\n",
    "        def __init__(self):\n",
    "            # mu ~ N(-2.5, 1^2)  \n",
    "            self.mu_prior = stats.norm(loc=-2.5, scale=1.0)\n",
    "            # phi ~ Beta(20, 2) scaled to (0,1) for stationarity\n",
    "            self.phi_prior = stats.beta(a=20, b=2)\n",
    "            # sigma_x ~ InvGamma(3, 0.5) \n",
    "            self.sigma_x_prior = stats.invgamma(a=3, scale=0.5)\n",
    "            \n",
    "        def logpdf(self, theta):\n",
    "            mu, phi, sigma_x = theta\n",
    "            if not (0 < phi < 1 and sigma_x > 0):\n",
    "                return -np.inf\n",
    "            return (self.mu_prior.logpdf(mu) + \n",
    "                   self.phi_prior.logpdf(phi) +\n",
    "                   self.sigma_x_prior.logpdf(sigma_x))\n",
    "        \n",
    "        def rvs(self, size=None):\n",
    "            mu = self.mu_prior.rvs(size)\n",
    "            phi = self.phi_prior.rvs(size) \n",
    "            sigma_x = self.sigma_x_prior.rvs(size)\n",
    "            if size is None:\n",
    "                return np.array([mu, phi, sigma_x])\n",
    "            else:\n",
    "                return np.column_stack([mu, phi, sigma_x])\n",
    "    \n",
    "    # MCMC sampler\n",
    "    class SVPosterior:\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "            self.prior = SVPrior()\n",
    "            \n",
    "        def logpdf(self, theta):\n",
    "            mu, phi, sigma_x = theta\n",
    "            \n",
    "            # Prior\n",
    "            log_prior = self.prior.logpdf(theta)\n",
    "            if np.isinf(log_prior):\n",
    "                return log_prior\n",
    "                \n",
    "            # Likelihood via particle filter\n",
    "            try:\n",
    "                model = StochasticVolatilityModel(mu=mu, phi=phi, sigma_x=sigma_x)\n",
    "                fk = ssm.Bootstrap(ssm=model, data=self.data)\n",
    "                pf = particles.SMC(fk=fk, N=100)  # Smaller N for MCMC\n",
    "                pf.run()\n",
    "                log_likelihood = pf.logLt\n",
    "                return log_prior + log_likelihood\n",
    "            except:\n",
    "                return -np.inf\n",
    "    \n",
    "    # Run MCMC\n",
    "    y_data = returns.values\n",
    "    posterior = SVPosterior(y_data)\n",
    "    \n",
    "    # Initial value\n",
    "    theta0 = np.array([-2.5, 0.95, 0.3])\n",
    "    \n",
    "    # MCMC chain\n",
    "    print(\"🔗 Running MCMC chain...\")\n",
    "    chain = []\n",
    "    current_theta = theta0\n",
    "    current_logpdf = posterior.logpdf(current_theta)\n",
    "    n_accept = 0\n",
    "    \n",
    "    # Proposal covariance (tuned)\n",
    "    prop_cov = np.diag([0.1, 0.01, 0.02])**2\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"   Iteration {i}/{n_iter}\")\n",
    "            \n",
    "        # Propose new state\n",
    "        proposal = np.random.multivariate_normal(current_theta, prop_cov)\n",
    "        proposal_logpdf = posterior.logpdf(proposal)\n",
    "        \n",
    "        # Accept/reject\n",
    "        log_alpha = proposal_logpdf - current_logpdf\n",
    "        if np.log(np.random.rand()) < log_alpha:\n",
    "            current_theta = proposal\n",
    "            current_logpdf = proposal_logpdf\n",
    "            n_accept += 1\n",
    "            \n",
    "        chain.append(current_theta.copy())\n",
    "    \n",
    "    print(f\"✓ MCMC completed. Acceptance rate: {n_accept/n_iter:.2%}\")\n",
    "    \n",
    "    # Process results\n",
    "    chain = np.array(chain)\n",
    "    burned_chain = chain[n_burn:]\n",
    "    \n",
    "    # Parameter estimates\n",
    "    param_estimates = {\n",
    "        'mu': {'mean': np.mean(burned_chain[:, 0]), 'std': np.std(burned_chain[:, 0])},\n",
    "        'phi': {'mean': np.mean(burned_chain[:, 1]), 'std': np.std(burned_chain[:, 1])},\n",
    "        'sigma_x': {'mean': np.mean(burned_chain[:, 2]), 'std': np.std(burned_chain[:, 2])}\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'chain': chain,\n",
    "        'burned_chain': burned_chain,\n",
    "        'estimates': param_estimates,\n",
    "        'acceptance_rate': n_accept/n_iter\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97d78b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chunk 7: Visualization Function\n",
    "# =============================================================================\n",
    "\n",
    "def create_comprehensive_plots(data, filtering_results, smoothing_results, mcmc_results):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of results\n",
    "    \"\"\"\n",
    "    print(\"📈 Creating visualizations...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('S&P 500 Stochastic Volatility Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    dates = data.index\n",
    "    returns = data['Returns']\n",
    "    \n",
    "    # 1. Returns time series\n",
    "    axes[0,0].plot(dates, returns, 'b-', alpha=0.7, linewidth=0.8)\n",
    "    axes[0,0].set_title('S&P 500 Daily Returns')\n",
    "    axes[0,0].set_ylabel('Returns (%)')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Filtering vs Smoothing\n",
    "    vol_filter = filtering_results['vol_estimates']\n",
    "    vol_smooth = smoothing_results['vol_estimates']\n",
    "    \n",
    "    axes[0,1].plot(dates, vol_filter, 'r-', label='Filtered', alpha=0.8)\n",
    "    axes[0,1].plot(dates, vol_smooth, 'g-', label='Smoothed', alpha=0.8)\n",
    "    axes[0,1].fill_between(dates, \n",
    "                          vol_filter - 1.96*filtering_results['vol_std'],\n",
    "                          vol_filter + 1.96*filtering_results['vol_std'],\n",
    "                          alpha=0.2, color='red', label='95% CI (Filter)')\n",
    "    axes[0,1].set_title('Volatility Estimates: Filtering vs Smoothing')\n",
    "    axes[0,1].set_ylabel('Annualized Volatility (%)')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Returns vs Volatility\n",
    "    axes[0,2].scatter(vol_filter[:-1], np.abs(returns[1:]), alpha=0.6, s=20)\n",
    "    axes[0,2].set_xlabel('Estimated Volatility (%)')\n",
    "    axes[0,2].set_ylabel('Absolute Returns (%)')\n",
    "    axes[0,2].set_title('Volatility vs Absolute Returns')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. MCMC trace plots\n",
    "    chain = mcmc_results['burned_chain']\n",
    "    param_names = ['μ (Long-run log-vol)', 'φ (Persistence)', 'σₓ (Vol-of-vol)']\n",
    "    \n",
    "    for i, param in enumerate(param_names):\n",
    "        axes[1,i].plot(chain[:, i], alpha=0.7)\n",
    "        axes[1,i].set_title(f'MCMC Trace: {param}')\n",
    "        axes[1,i].set_ylabel('Parameter Value')\n",
    "        axes[1,i].set_xlabel('Iteration')\n",
    "        axes[1,i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_val = np.mean(chain[:, i])\n",
    "        axes[1,i].axhline(y=mean_val, color='red', linestyle='--', \n",
    "                         label=f'Mean: {mean_val:.3f}')\n",
    "        axes[1,i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feac7828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chunk 8: Summary Statistics Function\n",
    "# =============================================================================\n",
    "\n",
    "def print_analysis_summary(data, filtering_results, smoothing_results, mcmc_results):\n",
    "    \"\"\"\n",
    "    Print comprehensive analysis summary\n",
    "    \"\"\"\n",
    "    dates = data.index\n",
    "    returns = data['Returns']\n",
    "    vol_filter = filtering_results['vol_estimates']\n",
    "    vol_smooth = smoothing_results['vol_estimates']\n",
    "    estimates = mcmc_results['estimates']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n📈 DATA SUMMARY:\")\n",
    "    print(f\"   Period: {dates[0].strftime('%Y-%m-%d')} to {dates[-1].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Trading days: {len(returns)}\")\n",
    "    print(f\"   Average return: {returns.mean():.3f}%\")\n",
    "    print(f\"   Return volatility: {returns.std():.2f}%\")\n",
    "    print(f\"   Annualized volatility: {returns.std() * np.sqrt(252):.1f}%\")\n",
    "    \n",
    "    print(f\"\\n🎯 VOLATILITY ESTIMATES:\")\n",
    "    print(f\"   Average filtered volatility: {vol_filter.mean():.1f}%\")\n",
    "    print(f\"   Average smoothed volatility: {vol_smooth.mean():.1f}%\")\n",
    "    print(f\"   Max volatility (filtered): {vol_filter.max():.1f}%\")\n",
    "    print(f\"   Min volatility (filtered): {vol_filter.min():.1f}%\")\n",
    "    \n",
    "    print(f\"\\n⚙️  PARAMETER ESTIMATES:\")\n",
    "    print(f\"   μ (long-run log-vol): {estimates['mu']['mean']:.3f} ± {estimates['mu']['std']:.3f}\")\n",
    "    print(f\"   φ (persistence): {estimates['phi']['mean']:.3f} ± {estimates['phi']['std']:.3f}\")  \n",
    "    print(f\"   σₓ (vol-of-vol): {estimates['sigma_x']['mean']:.3f} ± {estimates['sigma_x']['std']:.3f}\")\n",
    "    print(f\"   MCMC acceptance rate: {mcmc_results['acceptance_rate']:.1%}\")\n",
    "    \n",
    "    # Business interpretation\n",
    "    long_run_vol = np.exp(estimates['mu']['mean']/2) * np.sqrt(252)\n",
    "    persistence_days = -1 / np.log(estimates['phi']['mean'])\n",
    "    \n",
    "    print(f\"\\n💼 BUSINESS INTERPRETATION:\")\n",
    "    print(f\"   Long-run volatility: {long_run_vol:.1f}% (annualized)\")\n",
    "    print(f\"   Volatility shock half-life: {persistence_days:.1f} trading days\")\n",
    "    print(f\"   Volatility clustering: {'Strong' if estimates['phi']['mean'] > 0.9 else 'Moderate'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c629e299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Fetching S&P 500 data from 2023-01-01 to 2024-11-01...\n",
      "YF.download() has changed argument auto_adjust default to True\n",
      "✓ Downloaded 460 trading days\n",
      "✓ Return statistics: Mean=0.087%, Std=0.81%\n",
      "\n",
      "Data shape: (460, 6)\n",
      "Date range: 2023-01-04 00:00:00 to 2024-10-31 00:00:00\n",
      "Returns summary:\n",
      "count    460.000000\n",
      "mean       0.086976\n",
      "std        0.808087\n",
      "min       -3.042704\n",
      "25%       -0.333784\n",
      "50%        0.090485\n",
      "75%        0.606760\n",
      "max        2.278114\n",
      "Name: Returns, dtype: float64\n",
      "\n",
      "Initialized SV model with parameters:\n",
      "  μ = -2.5\n",
      "  φ = 0.95\n",
      "  σₓ = 0.3\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Chunk 9: Load Data and Initialize Model\n",
    "# =============================================================================\n",
    "\n",
    "# Fetch S&P 500 data\n",
    "data = fetch_sp500_data(start_date=\"2023-01-01\", end_date=\"2024-11-01\")\n",
    "returns = data['Returns']\n",
    "\n",
    "# Display basic data info\n",
    "print(f\"\\nData shape: {data.shape}\")\n",
    "print(f\"Date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Returns summary:\\n{returns.describe()}\")\n",
    "\n",
    "# Initialize stochastic volatility model with default parameters\n",
    "model = StochasticVolatilityModel(mu=-2.5, phi=0.95, sigma_x=0.3)\n",
    "print(f\"\\nInitialized SV model with parameters:\")\n",
    "print(f\"  μ = {model.mu}\")\n",
    "print(f\"  φ = {model.phi}\")  \n",
    "print(f\"  σₓ = {model.sigma_x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eeb7c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Running particle filter with 1000 particles...\n",
      "✓ Particle filtering completed\n",
      "\n",
      "Filtering Results Summary:\n",
      "  Average estimated volatility: 10.1%\n",
      "  Volatility range: 4.8% - 21.9%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Chunk 10: Run Particle Filter\n",
    "# =============================================================================\n",
    "\n",
    "# Run particle filtering for real-time volatility estimation\n",
    "filtering_results = run_particle_filter(model, returns, N=1000)\n",
    "\n",
    "print(f\"\\nFiltering Results Summary:\")\n",
    "print(f\"  Average estimated volatility: {filtering_results['vol_estimates'].mean():.1f}%\")\n",
    "print(f\"  Volatility range: {filtering_results['vol_estimates'].min():.1f}% - {filtering_results['vol_estimates'].max():.1f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7942f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Running particle smoother with 1000 particles...\n",
      "⚠️  Backward sampling failed: 'ParticleHistory' object has no attribute 'backward_sampling'\n",
      "   Using alternative smoothing approach...\n",
      "✓ Alternative smoothing completed\n",
      "\n",
      "Smoothing Results Summary:\n",
      "  Average smoothed volatility: 10.1%\n",
      "  Smoothed volatility range: 5.7% - 17.3%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Chunk 11: Run Particle Smoother\n",
    "# =============================================================================\n",
    "\n",
    "# Run particle smoothing for refined volatility estimates\n",
    "smoothing_results = run_particle_smoother(model, returns, N=1000)\n",
    "\n",
    "print(f\"\\nSmoothing Results Summary:\")\n",
    "print(f\"  Average smoothed volatility: {smoothing_results['vol_estimates'].mean():.1f}%\")\n",
    "print(f\"  Smoothed volatility range: {smoothing_results['vol_estimates'].min():.1f}% - {smoothing_results['vol_estimates'].max():.1f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f57091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Running Bayesian parameter estimation (2000 iterations)...\n",
      "🔗 Running MCMC chain...\n",
      "   Iteration 0/2000\n",
      "   Iteration 500/2000\n",
      "   Iteration 1000/2000\n",
      "   Iteration 1500/2000\n",
      "✓ MCMC completed. Acceptance rate: 43.45%\n",
      "\n",
      "MCMC Results Summary:\n",
      "  mu: -0.546 ± 0.127\n",
      "  phi: 0.942 ± 0.023\n",
      "  sigma_x: 0.149 ± 0.037\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Chunk 12: Bayesian Parameter Estimation\n",
    "# =============================================================================\n",
    "\n",
    "# Run MCMC for Bayesian parameter estimation\n",
    "# Note: This can take several minutes to complete\n",
    "mcmc_results = bayesian_parameter_estimation(returns, n_iter=2000, n_burn=500)\n",
    "\n",
    "print(f\"\\nMCMC Results Summary:\")\n",
    "for param, est in mcmc_results['estimates'].items():\n",
    "    print(f\"  {param}: {est['mean']:.3f} ± {est['std']:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chunk 13: Create Visualizations\n",
    "# =============================================================================\n",
    "\n",
    "# Create comprehensive plots\n",
    "fig = create_comprehensive_plots(data, filtering_results, smoothing_results, mcmc_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190e72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chunk 14: Print Final Summary\n",
    "# =============================================================================\n",
    "\n",
    "# Print comprehensive analysis summary\n",
    "print_analysis_summary(data, filtering_results, smoothing_results, mcmc_results)\n",
    "\n",
    "print(\"\\n✅ Analysis completed successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chunk 15: Store Results (Optional)\n",
    "# =============================================================================\n",
    "\n",
    "# Combine all results into a single dictionary for easy access\n",
    "results = {\n",
    "    'data': data,\n",
    "    'filtering': filtering_results,\n",
    "    'smoothing': smoothing_results,\n",
    "    'mcmc': mcmc_results,\n",
    "    'model': model\n",
    "}\n",
    "\n",
    "# Display final message\n",
    "print(f\"\\n🎉 All results stored in 'results' dictionary\")\n",
    "print(f\"   Available keys: {list(results.keys())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
